{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dl_utlils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dl_utlils.py\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "def relu(x):\n",
    "    return T.maximum(0, x)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return T.nnet.sigmoid(x)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return T.tanh(x)\n",
    "\n",
    "\n",
    "class Metric(object):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def negative_log_likelihood(self):\n",
    "        self.prob_of_y_given_x = T.nnet.softmax(self.x)\n",
    "        return -T.mean(T.log(self.prob_of_y_given_x)[T.arange(self.y.shape[0]), self.y])\n",
    "\n",
    "    def cross_entropy(self):\n",
    "        self.prob_of_y_given_x = T.nnet.softmax(self.x)\n",
    "        return T.mean(T.nnet.categorical_crossentropy(self.prob_of_y_given_x, self.y))\n",
    "\n",
    "    def mean_squared_error(self):\n",
    "        return T.mean((self.x - self.y) ** 2)\n",
    "\n",
    "    def errors(self):\n",
    "        if self.y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError('y should have the same shape as self.y_pred',\n",
    "                            ('y', self.y.type, 'y_pred', self.y_pred.type))\n",
    "\n",
    "        if self.y.dtype.startswith('int'):\n",
    "            self.prob_of_y_given_x = T.nnet.softmax(self.x)\n",
    "            self.y_pred = T.argmax(self.prob_of_y_given_x, axis=1)\n",
    "            return T.mean(T.neq(self.y_pred, self.y))\n",
    "        else:\n",
    "            return NotImplementedError()\n",
    "\n",
    "    def accuracy(self):\n",
    "        if self.y.dtype.startswith('int'):\n",
    "            self.prob_of_y_given_x = T.nnet.softmax(self.x)\n",
    "            self.y_pred = T.argmax(self.prob_of_y_given_x, axis=1)\n",
    "            return T.mean(T.eq(self.y_pred, self.y))\n",
    "        else:\n",
    "            return NotImplementedError()\n",
    "\n",
    "\n",
    "def shared_data(x, y):\n",
    "    shared_x = theano.shared(\n",
    "        np.asarray(x, dtype=theano.config.floatX), borrow=True)\n",
    "    if y is None:\n",
    "        return shared_x\n",
    "\n",
    "    shared_y = theano.shared(\n",
    "        np.asarray(y, dtype=theano.config.floatX), borrow=True)\n",
    "\n",
    "    return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "\n",
    "def build_shared_zeros(shape, name):\n",
    "    \"\"\" Builds a theano shared variable filled with a zeros numpy array \"\"\"\n",
    "    return theano.shared(\n",
    "        value=np.zeros(shape, dtype=theano.config.floatX),\n",
    "        name=name,\n",
    "        borrow=True\n",
    "    )\n",
    "\n",
    "\n",
    "def dropout(x, train, p=0.5, rng = np.random.RandomState(1234)):\n",
    "    masked_x = None\n",
    "    if p > 0.0 and p < 1.0:\n",
    "        seed = rng.randint(2 ** 30)\n",
    "        srng = T.shared_randomstreams.RandomStreams(seed)\n",
    "        mask = srng.binomial(\n",
    "            n=1,\n",
    "            p=1.0 - p,\n",
    "            size=x.shape,\n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "        masked_x = x * mask\n",
    "    else:\n",
    "        masked_x = x\n",
    "    return T.switch(T.neq(train, 0), masked_x, x * (1.0 - p))\n",
    "\n",
    "\n",
    "class Optimizer(object):\n",
    "\n",
    "    def __init__(self, params=None):\n",
    "        if params is None:\n",
    "            return NotImplementedError()\n",
    "        self.params = params\n",
    "\n",
    "    def updates(self, loss=None):\n",
    "        if loss is None:\n",
    "            return NotImplementedError()\n",
    "\n",
    "        self.updates = OrderedDict()\n",
    "        self.gparams = [T.grad(loss, param) for param in self.params]\n",
    "\n",
    "\n",
    "def build_shared_zeros(shape, name):\n",
    "    \"\"\" Builds a theano shared variable filled with a zeros numpy array \"\"\"\n",
    "    return theano.shared(\n",
    "        value=np.zeros(shape, dtype=theano.config.floatX),\n",
    "        name=name,\n",
    "        borrow=True\n",
    "    )\n",
    "\n",
    "\n",
    "class RMSprop(Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, alpha=0.99, eps=1e-8, params=None):\n",
    "        super(RMSprop, self).__init__(params=params)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "\n",
    "        self.mss = [\n",
    "            build_shared_zeros(t.shape.eval(), 'ms') for t in self.params]\n",
    "\n",
    "    def updates(self, loss=None):\n",
    "        super(RMSprop, self).updates(loss=loss)\n",
    "\n",
    "        for ms, param, gparam in zip(self.mss, self.params, self.gparams):\n",
    "            _ms = ms * self.alpha\n",
    "            _ms += (1 - self.alpha) * gparam * gparam\n",
    "            self.updates[ms] = _ms\n",
    "            self.updates[param] = param - self.learning_rate * \\\n",
    "                gparam / T.sqrt(_ms + self.eps)\n",
    "\n",
    "        return self.updates\n",
    "\n",
    "class AdaDelta(Optimizer):\n",
    "\n",
    "    def __init__(self, rho=0.95, eps=1e-6, params=None):\n",
    "        super(AdaDelta, self).__init__(params=params)\n",
    "\n",
    "        self.rho = rho\n",
    "        self.eps = eps\n",
    "        self.accugrads = [\n",
    "            build_shared_zeros(t.shape.eval(), 'accugrad') for t in self.params]\n",
    "        self.accudeltas = [\n",
    "            build_shared_zeros(t.shape.eval(), 'accudelta') for t in self.params]\n",
    "\n",
    "    def updates(self, loss=None):\n",
    "        super(AdaDelta, self).updates(loss=loss)\n",
    "\n",
    "        for accugrad, accudelta, param, gparam\\\n",
    "                in zip(self.accugrads, self.accudeltas, self.params, self.gparams):\n",
    "            agrad = self.rho * accugrad + (1 - self.rho) * gparam * gparam\n",
    "            dx = - T.sqrt((accudelta + self.eps) / (agrad + self.eps)) * gparam\n",
    "            self.updates[accudelta] = (\n",
    "                self.rho * accudelta + (1 - self.rho) * dx * dx)\n",
    "            self.updates[param] = param + dx\n",
    "            self.updates[accugrad] = agrad\n",
    "\n",
    "        return self.updates\n",
    "\n",
    "class MomentumSGD(Optimizer):\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9, params=None):\n",
    "        super(MomentumSGD, self).__init__(params=params)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.vs = [build_shared_zeros(t.shape.eval(), 'v')\n",
    "                   for t in self.params]\n",
    "\n",
    "    def updates(self, loss=None):\n",
    "        super(MomentumSGD, self).updates(loss=loss)\n",
    "\n",
    "        for v, param, gparam in zip(self.vs, self.params, self.gparams):\n",
    "            _v = v * self.momentum\n",
    "            _v = _v - self.learning_rate * gparam\n",
    "            self.updates[param] = param + _v\n",
    "            self.updates[v] = _v\n",
    "\n",
    "        return self.updates    \n",
    "\n",
    "class Adam(Optimizer):\n",
    "\n",
    "    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-8, gamma=1 - 1e-8, params=None):\n",
    "        super(Adam, self).__init__(params=params)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.b1 = beta1\n",
    "        self.b2 = beta2\n",
    "        self.gamma = gamma\n",
    "        self.t = theano.shared(np.float32(1))\n",
    "        self.eps = eps\n",
    "\n",
    "        self.ms = [build_shared_zeros(t.shape.eval(), 'm')\n",
    "                   for t in self.params]\n",
    "        self.vs = [build_shared_zeros(t.shape.eval(), 'v')\n",
    "                   for t in self.params]\n",
    "\n",
    "    def updates(self, loss=None):\n",
    "        super(Adam, self).updates(loss=loss)\n",
    "        self.b1_t = self.b1 * self.gamma ** (self.t - 1)\n",
    "\n",
    "        for m, v, param, gparam \\\n",
    "                in zip(self.ms, self.vs, self.params, self.gparams):\n",
    "            _m = self.b1_t * m + (1 - self.b1_t) * gparam\n",
    "            _v = self.b2 * v + (1 - self.b2) * gparam ** 2\n",
    "\n",
    "            m_hat = _m / (1 - self.b1 ** self.t)\n",
    "            v_hat = _v / (1 - self.b2 ** self.t)\n",
    "\n",
    "            self.updates[param] = param - self.alpha * \\\n",
    "                m_hat / (T.sqrt(v_hat) + self.eps)\n",
    "            self.updates[m] = _m\n",
    "            self.updates[v] = _v\n",
    "        self.updates[self.t] = self.t + 1.0\n",
    "\n",
    "        return self.updates\n",
    "\n",
    "# Multi Layer Perceptron\n",
    "\n",
    "class Layer:\n",
    "    # Constructor\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        rng = np.random.RandomState(1234)\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.W = theano.shared(rng.uniform(low=-0.08, high=0.08,\n",
    "                                           size=(in_dim, out_dim)\n",
    "                                           ).astype('float32'), name='W')\n",
    "        self.b = theano.shared(np.zeros(out_dim).astype('float32'), name='b')\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "\n",
    "    # Forward Propagation\n",
    "    def f_prop(self, x):\n",
    "        self.z = T.dot(x, self.W) + self.b\n",
    "        return self.z\n",
    "\n",
    "class Activation:\n",
    "    # Constructor\n",
    "    def __init__(self, function):\n",
    "        self.function = function\n",
    "        self.params = []\n",
    "\n",
    "    # Forward Propagation\n",
    "    def f_prop(self, x):\n",
    "        self.z = self.function(x)\n",
    "        return self.z\n",
    "    \n",
    "class BatchNorm:\n",
    "    # Constructor\n",
    "    def __init__(self, shape, epsilon=np.float32(1e-5)):\n",
    "        self.shape = shape\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.gamma = theano.shared(np.ones(self.shape, dtype=\"float32\"),\n",
    "                                   name=\"gamma\")\n",
    "        self.beta = theano.shared(np.zeros(self.shape, dtype=\"float32\"),\n",
    "                                  name=\"beta\")\n",
    "        self.params = [self.gamma, self.beta]\n",
    "\n",
    "    # Forward Propagation\n",
    "    def f_prop(self, x):\n",
    "        if x.ndim == 2:\n",
    "            mean = T.mean(x, axis=0, keepdims=True)\n",
    "            std = T.sqrt(T.var(x, axis=0, keepdims=True) + self.epsilon)\n",
    "        elif x.ndim == 4:\n",
    "            mean = T.mean(x, axis=(0, 2, 3), keepdims=True)\n",
    "            std = T.sqrt(T.var(x, axis=(0, 2, 3), keepdims=True) +\n",
    "                         self.epsilon)\n",
    "\n",
    "        normalized_x = (x - mean) / std\n",
    "        self.z = self.gamma * normalized_x + self.beta\n",
    "        return self.z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ray/anaconda/envs/pure_theano/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 775M (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from theano.tensor.nnet import conv2d\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "rng = np.random.RandomState(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist_X, mnist_y = shuffle(mnist.data.astype('float32'),\n",
    "                           mnist.target.astype('int32'),\n",
    "                           random_state=42)\n",
    "\n",
    "mnist_X = mnist_X / 255.0\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(mnist_X, mnist_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os \n",
    "base_folder = \"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_y = np.eye(10)[train_y].astype('int32')\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d0a592c9ab77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                         allow_input_downcast=True, name='train')\n\u001b[1;32m     34\u001b[0m valid = theano.function(inputs=[x, t], outputs=[cost, T.argmax(y, axis=1)],\n\u001b[0;32m---> 35\u001b[0;31m                         allow_input_downcast=True, name='valid')\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pure_theano/lib/python3.6/site-packages/theano/compile/function.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    324\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    327\u001b[0m     \u001b[0;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m# borrowed used defined inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pure_theano/lib/python3.6/site-packages/theano/compile/pfunc.py\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pure_theano/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1793\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                    \u001b[0moutput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1795\u001b[0;31m             defaults)\n\u001b[0m\u001b[1;32m   1796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pure_theano/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input_storage, trustme, storage_map)\u001b[0m\n\u001b[1;32m   1659\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m             _fn, _i, _o = self.linker.make_thunk(\n\u001b[0;32m-> 1661\u001b[0;31m                 input_storage=input_storage_lists, storage_map=storage_map)\n\u001b[0m\u001b[1;32m   1662\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlimit_orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pure_theano/lib/python3.6/site-packages/theano/gof/link.py\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m    697\u001b[0m         return self.make_all(input_storage=input_storage,\n\u001b[1;32m    698\u001b[0m                              \u001b[0moutput_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                              storage_map=storage_map)[:3]\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pure_theano/lib/python3.6/site-packages/theano/gof/vm.py\u001b[0m in \u001b[0;36mmake_all\u001b[0;34m(self, profiler, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[1;32m   1045\u001b[0m                                                  \u001b[0mcompute_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                                                  \u001b[0mno_recycling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                                                  impl=impl))\n\u001b[0m\u001b[1;32m   1048\u001b[0m                 \u001b[0mlinker_make_thunk_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mthunk_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lazy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pure_theano/lib/python3.6/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling, impl)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 return self.make_c_thunk(node, storage_map, compute_map,\n\u001b[0;32m--> 935\u001b[0;31m                                          no_recycling)\n\u001b[0m\u001b[1;32m    936\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNotImplementedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodNotDefined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                 \u001b[0;31m# We requested the c code, so don't catch the error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pure_theano/lib/python3.6/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mmake_c_thunk\u001b[0;34m(self, node, storage_map, compute_map, no_recycling)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Trying CLinker.make_thunk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         outputs = cl.make_thunk(input_storage=node_input_storage,\n\u001b[0;32m--> 839\u001b[0;31m                                 output_storage=node_output_storage)\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0mfill_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_input_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_output_filters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pure_theano/lib/python3.6/site-packages/theano/gof/cc.py\u001b[0m in \u001b[0;36mmake_thunk\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         cthunk, in_storage, out_storage, error_storage = self.__compile__(\n\u001b[1;32m   1189\u001b[0m             \u001b[0minput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m             keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CThunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcthunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pure_theano/lib/python3.6/site-packages/theano/gof/cc.py\u001b[0m in \u001b[0;36m__compile__\u001b[0;34m(self, input_storage, output_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                                     \u001b[0moutput_storage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                                     \u001b[0mstorage_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m                                     keep_lock=keep_lock)\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return (thunk,\n\u001b[1;32m   1133\u001b[0m                 [link.Container(input, storage) for input, storage in\n",
      "\u001b[0;32m~/anaconda/envs/pure_theano/lib/python3.6/site-packages/theano/gof/cc.py\u001b[0m in \u001b[0;36mcthunk_factory\u001b[0;34m(self, error_storage, in_storage, out_storage, storage_map, keep_lock)\u001b[0m\n\u001b[1;32m   1601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         ret = module.instantiate(error_storage,\n\u001b[0;32m-> 1603\u001b[0;31m                                  *(in_storage + out_storage + orphd))\n\u001b[0m\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "from dl_utlils import *\n",
    "activation = relu\n",
    "mlp_layers =  [784, 500, 500, 500, 10]\n",
    "layers = []\n",
    "for i_layer in range(len(mlp_layers)-2):\n",
    "    layers.append(Layer(mlp_layers[i_layer], mlp_layers[i_layer+1]))\n",
    "    # BatchNorm(mlp_layers[i_layer+1],mlp_layers[i_layer+1])\n",
    "    layers.append(Activation(relu))\n",
    "    \n",
    "layers.append(Layer(mlp_layers[-2], mlp_layers[-1]))\n",
    "# layers.append(Activation(T.nnet.softmax))\n",
    "    \n",
    "x = T.fmatrix('x')\n",
    "t = T.imatrix('t')\n",
    "\n",
    "params = []\n",
    "for i, layer in enumerate(layers):\n",
    "    params += layer.params\n",
    "    if i == 0:\n",
    "        layer_out = layer.f_prop(x)\n",
    "    else:\n",
    "        layer_out = layer.f_prop(layer_out)\n",
    "\n",
    "y = layers[-1].z\n",
    "# cost = T.mean(T.nnet.categorical_crossentropy(y, t))\n",
    "cost = T.mean((y - t) ** 2)\n",
    "optimizer = Adam(params=params)\n",
    "updates = optimizer.updates(cost)\n",
    "\n",
    "train = theano.function(inputs=[x, t], outputs=cost, updates=updates,\n",
    "                        allow_input_downcast=True, name='train')\n",
    "valid = theano.function(inputs=[x, t], outputs=[cost, y],#T.argmax(y, axis=1)],\n",
    "                        allow_input_downcast=True, name='valid')\n",
    "test = theano.function(inputs=[x], outputs=T.argmax(y, axis=1), name='test')\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = train_X.shape[0]//batch_size\n",
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    #train_X, train_y = shuffle(train_X, train_y)\n",
    "    for i in range(n_batches):\n",
    "        start = i*batch_size\n",
    "        end = start + batch_size\n",
    "        train(train_X[start:end], train_y[start:end])\n",
    "    valid_cost, pred_y = valid(valid_X, valid_y)\n",
    "    print('EPOCH:: %i, Validation cost: %.3f, Validation F1: %.3f' %\n",
    "          (epoch + 1, valid_cost,\n",
    "           f1_score(np.argmax(valid_y, axis=1).astype('int32'),\n",
    "                    pred_y, average='macro')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
